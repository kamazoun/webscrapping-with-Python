{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "webscrapping_customs_data.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kamazoun/webscrapping-with-Python/blob/master/webscrapping_customs_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "iy9lkKc4Rzgj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def process_details(details):\n",
        "  try:\n",
        "    ls = details.split(\"\\n\")\n",
        "    fls = []\n",
        "    for l in ls:\n",
        "      l = l.strip()\n",
        "      if l != \"\": fls.append(l)\n",
        "    return fls[0], fls[1], fls[2]\n",
        "  except:\n",
        "    return \"\", \"\", details\n",
        "\n",
        "def process_destination(dest):\n",
        "  try:\n",
        "    ls = dest.split(\"\\n\")\n",
        "    fls = []\n",
        "    for l in ls:\n",
        "      l = l.strip()\n",
        "      if l != \"\": fls.append(l)\n",
        "    return fls[0], fls[1]\n",
        "  except:\n",
        "    return \"\", dest"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ylDwSJm5DWRw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "export_list = []\n",
        "import_list = []\n",
        "\n",
        "def extract_exports(info):\n",
        "  d  = {}\n",
        "  for row in info:\n",
        "    d[\"exporter\"] = row.find_all(\"div\", {\"class\": \"col-xs-2\"})[0].text.strip() #useless, exporter is company\n",
        "    d[\"importer\"] = row.find_all(\"div\", {\"class\": \"col-xs-2\"})[1].text.strip()\n",
        "    details = row.find(\"div\", {\"class\": \"col-xs-5\"}).text.strip()\n",
        "    details_ls = []\n",
        "    d[\"detail 1\"], d[\"detail 2\"], d[\"detail 3\"] = process_details(details) # a list\n",
        "    d[\"Departure\"] = row.find_all(\"div\", {\"class\": \"col-xs-6\"})[0].text.strip()\n",
        "    destination = row.find_all(\"div\", {\"class\": \"col-xs-6\"})[1].text.strip()\n",
        "    d[\"Arrival date\"], d[\"Destination\"] = process_destination(destination)\n",
        "    \n",
        "    export_list.append(d)\n",
        "  \n",
        "\n",
        "  \n",
        "def extract_imports(info):\n",
        "  d = {}\n",
        "  for row in info:\n",
        "    d[\"exporter\"] = row.find_all(\"div\", {\"class\": \"col-xs-2\"})[0].text.strip()\n",
        "    d[\"importer\"] = row.find_all(\"div\", {\"class\": \"col-xs-2\"})[1].text.strip() #useless, exporter is company\n",
        "    details = row.find(\"div\", {\"class\": \"col-xs-5\"}).text.strip()\n",
        "    details_ls = []\n",
        "    d[\"detail 1\"], d[\"detail 2\"], d[\"detail 3\"] = process_details(details) # a list\n",
        "    d[\"Departure\"] = row.find_all(\"div\", {\"class\": \"col-xs-6\"})[0].text.strip()\n",
        "    destination = row.find_all(\"div\", {\"class\": \"col-xs-6\"})[1].text.strip()\n",
        "    d[\"Arrival date\"], d[\"Destination\"] = process_destination(destination)\n",
        "    \n",
        "    import_list.append(d)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G_w5VMXUw0Ji",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def urls_in_page(page):\n",
        "  r = requests.get(page)\n",
        "  c = r.content\n",
        "  soup = BeautifulSoup(c, \"html.parser\")\n",
        "  all_links = soup.find_all(\"ul\", {\"class\": \"directory-results directory-results-3column\"})\n",
        "  hrefs = []\n",
        "  for links in all_links:\n",
        "    try:\n",
        "      l = links.find_all(\"a\")\n",
        "    except:\n",
        "      return []\n",
        "    for link in l:\n",
        "      hrefs.append(link.get('href')) \n",
        "  return hrefs   \n",
        "\n",
        "\n",
        "\n",
        "def link_pages():\n",
        "  base_url = \"http://www.tradesparq.com/company-directory/\"\n",
        "  page = []\n",
        "  for i in range(0, 150):\n",
        "    for j in range(0, 150):\n",
        "      page.append(base_url + str(i + 1) + \"/\" + str(j + 1))\n",
        "   \n",
        "  return page"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FUsczYczyCjv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "exports_data = []\n",
        "#pages = link_pages()\n",
        "\n",
        "counter = 0\n",
        "for page in link_pages():\n",
        "  #hrefs = urls_in_page(page)\n",
        "  for href in urls_in_page(page):\n",
        "    try:\n",
        "      r = requests.get(href)\n",
        "      c = r.content\n",
        "      soup = BeautifulSoup(c, \"html.parser\")\n",
        "\n",
        "      exports = soup.find_all(\"section\", {\"class\": \"shipments-section\"})[0].find_all(\"div\", {\"class\": \"shipment-record row\"})\n",
        "      imports = soup.find_all(\"section\", {\"class\": \"shipments-section\"})[1].find_all(\"div\", {\"class\": \"shipment-record row\"})\n",
        "    except:\n",
        "      print(\"error at counter \" + str(counter))\n",
        "      print(\"\\nPage : \" + href)\n",
        "      pass\n",
        "    \n",
        "    extract_exports(exports)\n",
        "    extract_imports(imports)\n",
        "  \n",
        "    counter = counter + 1\n",
        "  \n",
        "  \n",
        "df_export = pd.DataFrame(export_list)\n",
        "df_import = pd.DataFrame(import_list)\n",
        "#df\n",
        "df_export.to_excel(\"export_output.xlsx\")\n",
        "df_import.to_excel(\"import_output.xlsx\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}